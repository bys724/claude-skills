---
name: paper-summary
description: Interactive academic paper reading, discussion, and comprehensive note generation. Creates a single detailed note combining personal context, objective content, and discussion highlights for Obsidian. The note can later be processed by another Claude instance for reorganization. Use when researcher wants to discuss and deeply document papers.
---

# Paper Summary Skill

A comprehensive skill for interactive paper reading, discussion, and unified personalized documentation.

## Overview

This skill helps researchers:
- Read and discuss academic papers interactively
- Create a single comprehensive note that includes:
  - **Personal context**: Why read this? What did I expect?
  - **Objective content**: Core ideas, methodology, results
  - **Discussion highlights**: Questions asked, papers compared, topics explored
  - **Personal evaluation**: Honest assessment, insights, connections to own research
  - **Related papers**: Network of relationships using typed links
- Organize papers with searchable tags
- Provide rich, detailed notes that can be later reorganized by another Claude instance

## Workflow

### Phase 1: Interactive Discussion

**User provides:**
- Paper link or file
- (Optional) Initial questions or points of interest

**Claude's role:**
- Read and understand the paper
- Engage in free-form discussion
- Listen to researcher's questions, interests, and interpretations
- **Do NOT create any output documents during this phase**
- **Continuously check**: "Would you like to proceed with the final summary?"

### Phase 2: Comprehensive Note Generation

When the user confirms readiness, Claude creates **ONE** comprehensive note that will serve as the complete record of this paper.

**CRITICAL: Before generating output, READ the template.md file in this skill directory for detailed format specifications and examples.**

**Output Characteristics**:
- Single unified document combining personal and objective elements
- Ready to paste into Obsidian
- Rich enough for future Claude instance to reorganize/refine
- Includes YAML frontmatter with metadata
- Flexible structure adapted to the discussion

**Content Scope** - The note must include:

1. **ğŸ“Œ Reading Context**
   - Why this paper was read
   - What was expected to learn
   - Connection to current research interests

2. **ğŸ¯ Core Ideas** (Objective)
   - One-sentence summary
   - Main contributions (3-5 bullet points)
   - Key approach with technical details

3. **ğŸ”¬ Methodology Details**
   - Architecture/algorithm specifics
   - Data and experimental setup
   - Implementation details discussed
   - Flexible subsections based on discussion depth

4. **ğŸ“Š Results & Analysis**
   - Key experimental findings
   - Ablation studies
   - Acknowledged limitations

5. **ğŸ’¬ Discussion Highlights**
   - Questions asked during conversation
   - Papers compared and discussed
   - Topics explored in depth

6. **ğŸ’­ Personal Evaluation & Insights**
   - Honest assessment (strengths, weaknesses, novelty, impact)
   - Connections to own research
   - New insights and ideas sparked
   - What stood out

7. **ğŸ”— Related Papers** (Typed Links)
   - Essential papers only (3-5 max)
   - Use Juggle format: `[[Paper|builds-on]]`, `[[Paper|vs]]`, `[[Paper|ref]]`
   - Include specific explanations

8. **â“ Questions & Future Exploration**
   - Unresolved questions
   - Papers to read next
   - Experiments or applications to try

**Tag Guidelines**:
- Use 3-5 general English keywords for classification
- Focus on searchable categories: research area, method, domain
- Examples: Robot, Transformer, Imitation Learning, Vision, Manipulation, VLA
- Classification over specificity

**Typed Link Criteria**:
- **builds-on**: Directly uses code/model/data or builds directly upon it
- **vs**: Direct experimental comparison with numbers/benchmarks
- **ref**: Referenced for background or context
- Only include papers explicitly discussed

### Phase 3: Future Usage (Paper Organization & Retrieval)

Based on accumulated summaries:
- Request paper classification by specific topics or perspectives
- Get recommendations for papers related to new research directions
- Analyze connections and citation networks between papers

## Claude's Core Responsibilities

1. **Paper comprehension**: Read and understand the provided paper
2. **Discussion engagement**: Discuss researcher's questions and interests deeply
3. **Template reading**: BEFORE generating final output, read `template.md` for format specifications
4. **Comprehensive note creation**: Generate a single unified note combining personal context, objective content, and discussion highlights
5. **Progress checking**: Continuously confirm when to proceed with final output

## Note Characteristics

The comprehensive note should be:
- **Personal + Objective**: Blends subjective evaluation with factual technical content
- **Discussion-driven**: Heavily reflects what was actually discussed and explored
- **Rich in context**: Includes motivations, expectations, and connections to researcher's work
- **Honest evaluation**: Contains frank assessment of strengths, weaknesses, and significance
- **Technically detailed**: Provides enough depth on methodology and results
- **Network-aware**: Shows relationships to other papers with typed links
- **Future-oriented**: Captures questions, ideas, and next steps
- **Flexible structure**: Adapts template sections based on conversation flow
- **Reorganization-ready**: Contains enough detail for future Claude to refine/restructure

## Starting a Session

Begin new sessions with:
"Let's start the paper review process. Please provide the paper link or file, and we'll discuss it together. When you're ready, I'll create a comprehensive note for Obsidian that combines your personal context, the paper's content, and our discussion highlights."


## Additional Recommendations (with final summary)

Only recommend 1-2 papers when they meet these criteria:
- Directly related to researcher's core questions, OR
- Essential for understanding the paper's core ideas, OR
- High-impact papers in robot control field

**Recommendation format**:
"Additional reference: [Paper title or reference number] - [Brief reason to read it]"

**Important**: The researcher already has many papers to read. Don't recommend low-importance papers. Omit recommendations if there are no pressing questions or if the paper has limited impact.

## Discussion Progress Check

Regularly ask during discussion:
"Is there anything else you'd like to discuss? Or shall we proceed with creating the comprehensive note based on our conversation so far?"

## Output Delivery Format

**BEFORE generating output**: Use the Read tool to read `template.md` in this skill directory for complete format specifications.

When delivering the final note, use this format:

```
## ğŸ“ Paper Note

[Complete comprehensive note with YAML frontmatter and all sections - copy-paste ready]
```

**Important**:
- Output is plain markdown (no code blocks wrapping the content)
- Include YAML frontmatter with title, tags, read_date, paper_type
- Use proper markdown syntax (headers, bold, lists, links)
- Ready to paste directly into Obsidian
- Follow template.md structure but adapt to discussion

## Output Example (Brief)

Here's a simplified example of how the comprehensive note should look:

```markdown
---
title: Attention Is All You Need
tags: [Transformer, NLP, Deep Learning, Attention Mechanism, Sequence-to-Sequence]
read_date: 2025-01-15
paper_type: Conference
---

# Attention Is All You Need

## ğŸ“Œ Reading Context

**ì™œ ì´ ë…¼ë¬¸ì„ ì½ì—ˆëŠ”ê°€?**
ìµœê·¼ Vision Transformer ê´€ë ¨ ì—°êµ¬ë¥¼ í•˜ë©´ì„œ Transformerì˜ ê¸°ë³¸ ì›ë¦¬ë¥¼ ì •í™•íˆ ì´í•´í•  í•„ìš”ê°€ ìˆì—ˆë‹¤. íŠ¹íˆ self-attentionì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€, ì™œ ì´ê²ƒì´ íš¨ê³¼ì ì¸ì§€ ì•Œê³  ì‹¶ì—ˆë‹¤. ë˜í•œ ë¡œë´‡ ë¹„ì „ ë¶„ì•¼ì— Transformerë¥¼ ì ìš©í•˜ëŠ” ìµœê·¼ ì—°êµ¬ë“¤ì„ ì´í•´í•˜ê¸° ìœ„í•œ ê¸°ì´ˆë¥¼ ë‹¤ì§€ê³ ì í–ˆë‹¤.

**ê¸°ëŒ€í–ˆë˜ ê²ƒ**
Multi-head attentionì˜ ì‘ë™ ì›ë¦¬, positional encodingì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„ ë°©ì‹, ê·¸ë¦¬ê³  ì™œ Transformerê°€ RNNì„ ëŒ€ì²´í•  ìˆ˜ ìˆì—ˆëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ì´í•´ë¥¼ ì–»ê³ ì í–ˆë‹¤.

---

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ (Core Ideas)

**í•œ ë¬¸ì¥ ìš”ì•½**
RNN/CNN ì—†ì´ ì˜¤ì§ attention mechanismë§Œìœ¼ë¡œ sequence-to-sequence ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ ë³‘ë ¬í™”ì™€ ì„±ëŠ¥ì„ ë™ì‹œì— ë‹¬ì„±í•œ ì•„í‚¤í…ì²˜.

**ì£¼ìš” ê¸°ì—¬ (Main Contributions)**
- Self-attentionë§Œìœ¼ë¡œ êµ¬ì„±ëœ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì œì•ˆ
- ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ì—¬ í•™ìŠµ ì‹œê°„ ëŒ€í­ ë‹¨ì¶•
- ê¸°ê³„ ë²ˆì—­ì—ì„œ SOTA ì„±ëŠ¥ ë‹¬ì„±
- ì´í›„ NLP ì „ë°˜ì˜ í‘œì¤€ ì•„í‚¤í…ì²˜ë¡œ ìë¦¬ì¡ìŒ

**í•µì‹¬ ì ‘ê·¼ë²• (Key Approach)**
ê¸°ì¡´ RNN/LSTMì€ sequential processingìœ¼ë¡œ ì¸í•´ ë³‘ë ¬í™”ê°€ ì–´ë µê³  long-range dependencyë¥¼ ì˜ í¬ì°©í•˜ì§€ ëª»í•œë‹¤. TransformerëŠ” ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œë²ˆì— ë³´ëŠ” self-attentionì„ ì‚¬ìš©í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤. Query, Key, Valueì˜ ê°œë…ìœ¼ë¡œ ê° í† í°ì´ ë‹¤ë¥¸ ëª¨ë“  í† í°ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ë©°, multi-headë¥¼ í†µí•´ ë‹¤ì–‘í•œ representation subspaceì—ì„œ attentionì„ ê³„ì‚°í•œë‹¤.

---

## ğŸ”¬ ë°©ë²•ë¡  ìƒì„¸ (Methodology Details)

### Multi-Head Attention
í† ë¡  ì¤‘ ì™œ 8ê°œì˜ headë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì§ˆë¬¸í–ˆë‹¤. ê° headê°€ ì„œë¡œ ë‹¤ë¥¸ representation subspaceë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ attentionì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ headëŠ” êµ¬ë¬¸ì  ê´€ê³„ë¥¼, ë‹¤ë¥¸ headëŠ” ì˜ë¯¸ì  ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

### Positional Encoding
Positional encodingì€ sin/cos í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´ë¥¼ ëª¨ë¸ì´ ì™¸ì‚½(extrapolate)í•  ìˆ˜ ìˆê²Œ í•œë‹¤. í•™ìŠµëœ positional embeddingë³´ë‹¤ ì¼ë°˜í™”ê°€ ì˜ ëœë‹¤ëŠ” ì ì„ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸í–ˆë‹¤.

---

## ğŸ“Š ê²°ê³¼ ë° ë¶„ì„ (Results & Analysis)

**ì£¼ìš” ì‹¤í—˜ ê²°ê³¼**
WMT 2014 English-to-Germanì—ì„œ BLEU 28.4, English-to-Frenchì—ì„œ 41.8ì„ ë‹¬ì„±í•˜ë©° ë‹¹ì‹œ SOTA ê°±ì‹ . í•™ìŠµ ì‹œê°„ì€ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ 1/10 ìˆ˜ì¤€.

**í•œê³„ì  (Limitations)**
ê¸´ ì‹œí€€ìŠ¤ì—ì„œ O(nÂ²) ë³µì¡ë„ê°€ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤. ì´ë¯¸ì§€ë‚˜ ì˜¤ë””ì˜¤ ê°™ì€ ì—°ì†ì  ë°ì´í„°ì— ë°”ë¡œ ì ìš©í•˜ê¸°ëŠ” ì–´ë µë‹¤.

---

## ğŸ’¬ Discussion Highlights

**í† ë¡  ì¤‘ ë‚˜ì˜¨ ì§ˆë¬¸ë“¤**
- Multi-headì˜ ê°œìˆ˜ëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜? â†’ Ablation study ì°¸ê³ 
- Positional encoding vs learned embedding ì°¨ì´ëŠ”? â†’ ì¼ë°˜í™” ì„±ëŠ¥ ì°¨ì´
- Visionì—ëŠ” ì–´ë–»ê²Œ ì ìš©? â†’ Patch ê¸°ë°˜ ì ‘ê·¼

**ë¹„êµ ë…¼ì˜í•œ ë…¼ë¬¸ë“¤**
Seq2Seq with Attentionê³¼ ë¹„êµí•˜ë©° ì–´ë–¤ ì ì´ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ì§€ í† ë¡ . GNMTì™€ì˜ ì„±ëŠ¥ ë¹„êµë„ ì‚´í´ë´„.

---

## ğŸ’­ Personal Evaluation & Insights

**ë…¼ë¬¸ì— ëŒ€í•œ í‰ê°€**
- **Strengths**: ê°„ê²°í•˜ë©´ì„œë„ ê°•ë ¥í•œ ì•„í‚¤í…ì²˜. ë³‘ë ¬í™” ê°€ëŠ¥ì„±ì´ ì‹¤ìš©ì ìœ¼ë¡œ ë§¤ìš° ì¤‘ìš”.
- **Weaknesses**: O(nÂ²) ë³µì¡ë„ê°€ ë§¤ìš° ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë¬¸ì œ.
- **Novelty**: Attentionì„ ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‚¬ìš©í•œ ê²ƒì€ í˜ì‹ ì .
- **Impact**: NLP íŒ¨ëŸ¬ë‹¤ì„ì„ ì™„ì „íˆ ë°”ê¿ˆ.

**ë‚´ ì—°êµ¬ì™€ì˜ ì—°ê²°ì **
ë¡œë´‡ ë¹„ì „ì— Transformerë¥¼ ì ìš©í•  ë•Œ ì´ë¯¸ì§€ë¥¼ patchë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ NLPì˜ í† í°í™”ì™€ ìœ ì‚¬í•˜ë‹¤. Inductive biasê°€ ì ë‹¤ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ë‹¤ì–‘í•œ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥í•œ ì¥ì ì´ ë  ìˆ˜ ìˆë‹¤.

**ìƒˆë¡œìš´ í†µì°° ë° ì•„ì´ë””ì–´**
Attention map ì‹œê°í™”ë¥¼ í†µí•´ ëª¨ë¸ì´ ì‹¤ì œë¡œ ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ì§€ ë¶„ì„í•˜ë©´ ë¡œë´‡ì˜ ì‹œê°ì  ì¶”ë¡  ê³¼ì •ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒ ê°™ë‹¤.

---

## ğŸ”— Related Papers & References

**ê´€ë ¨ ë…¼ë¬¸ (Typed Links)**
- [[Sequence to Sequence Learning with Neural Networks|builds-on]] - Transformerì˜ ê¸°ë°˜ì´ ë˜ëŠ” encoder-decoder êµ¬ì¡°ë¥¼ ì œì•ˆ
- [[GNMT|vs]] - ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ê³¼ í•™ìŠµ ì‹œê°„ì„ ì§ì ‘ ë¹„êµ
- [[Effective Approaches to Attention-based Neural Machine Translation|ref]] - Attention mechanismì˜ ê¸°ë³¸ ê°œë… ì œê³µ

---

## â“ Questions & Future Exploration

**ë¯¸í•´ê²° ì§ˆë¬¸**
- Sparse attention ê°™ì€ ë³€í˜•ë“¤ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ì¸ê°€?
- ë¡œë´‡ ì œì–´ ê°™ì€ ì—°ì†ì  í–‰ë™ ê³µê°„ì—ë„ ì ìš© ê°€ëŠ¥í•œê°€?

**í–¥í›„ ì½ì„ ë…¼ë¬¸**
- Vision Transformer (ViT)
- BERT (bidirectional transformer)
- Decision Transformer (RL ì ìš©)

**ì‹œë„í•´ë³¼ ê²ƒ**
- ë¡œë´‡ ì¡°ì‘ ë°ì´í„°ì— Transformer ì ìš© ì‹¤í—˜
- Attention weight ì‹œê°í™” ë„êµ¬ êµ¬í˜„
```

For complete templates and detailed guidelines, refer to `template.md`.
